---
title: "EDS_231_assignment_1: New York Times API"
author: "Marie Rivers"
date: "4/7/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# New York Times API

```{r message=FALSE, warning=FALSE}
library(jsonlite) #convert results from API queries into R-friendly formats 
library(tidyverse) 
library(tidytext) #text data management and analysis
library(ggplot2) #plot word frequencies and publication dates
library(here)
```

## Connect to the New York Times API and send a query
```{r}
#create an object called t with the results of the query ("xxx")
# the from JSON flatten the JSON object, then convert to a data frame
api_key <- "GW3whn8dTpvpdcAD1AIAiUMix03szFDn" # article search api
secret <- "RULf3NTWSXSB46um" # xxx
q <- "pfas" # query term
nyt_search_url <- "http://api.nytimes.com/svc/search/v2/articlesearch.json"
api_query <-paste0(nyt_search_url, "?q=", q, "&api-key=", api_key)
t <- fromJSON(api_query, flatten = TRUE)
```

```{r}
class(t) # t is currently a list
```

```{r}
# convert t to a dataframe
t <- t %>% 
  data.frame()
```

```{r}
#Inspect the data
class(t) # t is now a data frame
dim(t) # how big is it?
names(t) # what variables are we working with?
#t <- readRDS("nytDat.rds") #in case of API emergency :)
```

## Look at a piece of text. 
The data object has a variable called “response.docs.snippet” that contains a short excerpt, or “snippet” from the article. Grab a snippet and try out some basic ‘stringr’ functions.

```{r}
# xxx...was this supposed to return anythin???
t$response.docs.snippet[9]
# $response refers to the 'response column' from the api results
# snippets (from the NY Times) are sentences. `snippet[9]` pulls the 9th sentence
# try `response.docs.lead_paragraph` to get paragraphs

#assign a snippet to x to use as fodder for stringr functions.
```


```{r}
# xxx
x <- "Her nomination as secretary of the interior is historic, but as the first Native cabinet member, she would have to strike a delicate balance." 

tolower(x)
str_split(x, ','); str_split(x, 't')
str_replace(x, 'historic', 'without precedent')
str_replace(x, ' ', '_') #first one
#how do we replace all of them?

str_detect(x, 't'); str_detect(x, 'tive') ### is pattern in the string? T/F
str_locate(x, 't'); str_locate_all(x, 'as')
```

## Set parameters for a larger query
```{r}
term <- "pfas" # Need to use + to string together separate words
begin_date <- "20120101" # YYYYMMDD
end_date <- "20220101" #YYYYMMDD

#construct the query url using API operators
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",
                  term, "&begin_date=",begin_date, "&end_date=", end_date,
                  "&facet_filter=true&api-key=", api_key, sep="")
```

```{r}
#this code allows for obtaining multiple pages of query results 
initialQuery <- fromJSON(baseurl)
maxPages <- round((initialQuery$response$meta$hits[1] / 10) - 1) # might time out at 8 or 9 pages

pages <- list()
for(i in 0:maxPages){
  nytSearch <- fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame() 
  message("Retrieving page ", i)
  pages[[i+1]] <- nytSearch 
  Sys.sleep(6) # change Sys.sleep to 6
}
```

```{r}
class(nytSearch)
```

```{r}
#need to bind the pages and create a tibble from nytDa
nytDat <- rbind_pages(pages)
class(nytDat)
```

```{r}
# this might be a good place to export to csv as a backup in case the api times out
write_csv(nytDat, here("data", "nytDat.csv"))
```

```{r}
# xxx...backup
#nytDat <- read_csv("data/nyDat.csv")
```

```{r}
nytDat %>% 
  group_by(response.docs.type_of_material) %>%
  summarize(count=n()) %>%
  mutate(percent = (count / sum(count))*100) %>%
  ggplot() +
  geom_bar(aes(y=percent, x=response.docs.type_of_material, fill=response.docs.type_of_material), stat = "identity") + coord_flip() +
  labs(x = "Type of Material", fill = "Type of Material")
```

```{r}
nytDat %>%
  mutate(pubDay=gsub("T.*","",response.docs.pub_date)) %>%
  group_by(pubDay) %>%
  summarise(count=n()) %>%
  filter(count >= 1) %>% # change this if needed, based on search results
  ggplot() +
  geom_bar(aes(x=reorder(pubDay, count), y=count), stat="identity") + coord_flip() +
  labs(x = "publication date")
```

The New York Times doesn’t make full text of the articles available through the API. But we can use the first paragraph of each article. The NY Times includes 33 variables (paragraph, author info,...) Add a 34th column for 'word' as part of the unnesting process (started as 1 row per paragraph, then make a row per word)
```{r}
names(nytDat)
```

```{r}
paragraph <- names(nytDat)[6] #The 6th column, "response.doc.lead_paragraph", is the one we want here.  
tokenized <- nytDat %>%
  unnest_tokens(word, paragraph)
```

```{r}
names(tokenized)
```

```{r}
tokenized %>%
  count(word, sort = TRUE) %>%
  filter(n > 5) %>% #illegible with all the words displayed, consider increasing threshold to 10
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```

## remove stop words
```{r}
data(stop_words)

tokenized <- tokenized %>%
  anti_join(stop_words)

tokenized %>%
  count(word, sort = TRUE) %>%
  filter(n > 3) %>% # adjust this based on results
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```
Several steps were taken to clean tokens. Words such as 'administration's' and 'biden's' were cleaned to remove the 's. Numbers, which included single numbers and years, were removed.

```{r}
clean_tokens <- str_remove_all(tokenized$word, "[:digit:]") #remove all numbers
clean_tokens <- gsub("’s", '', clean_tokens) # gsub is used to replace all teh matches of a pattern from a string
#clean_tokens <- str_replace_all(clean_tokens,"land[a-z,A-Z]*","land") #stem tribe words
#clean_tokens <- str_remove_all(clean_tokens, "washington")

tokenized$clean <- clean_tokens
```

```{r}
tokenized %>%
  count(clean, sort = TRUE) %>%
  filter(n > 2) %>% # adjust based on results
  mutate(clean = reorder(clean, n)) %>%
  ggplot(aes(n, clean)) +
  geom_col() +
  labs(y = NULL)
```

```{r}
#remove the empty strings
tib <-subset(tokenized, clean!="")

#reassign
tokenized <- tib
```

```{r}
#try again
tokenized %>%
  count(clean, sort = TRUE) %>%
  filter(n > 2) %>% # adjust based on results
  mutate(clean = reorder(clean, n)) %>%
  ggplot(aes(n, clean)) +
  geom_col() +
  labs(y = NULL)
```