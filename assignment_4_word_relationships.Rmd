---
title: 'Assignment 4: Word Relationships'
author: "Marie Rivers"
date: "4/27/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages, message=FALSE, warning=FALSE}
library(tidyr) #text analysis in R
library(pdftools)
library(lubridate) #working with date data
library(tidyverse)
library(tidytext)
library(readr)
library(quanteda)
library(readtext) #quanteda subpackage for reading pdf
library(quanteda.textstats)
library(quanteda.textplots)
library(ggplot2)
library(forcats)
library(stringr)
library(quanteda.textplots)
library(widyr)# pairwise correlations
library(igraph) #network plots
library(ggraph)
library(here)
```

```{r pdf_import}
files <- list.files(path = here("data/EJ"),
                    pattern = "pdf$", full.names = TRUE)

ej_reports <- lapply(files, pdf_text)

ej_pdf <- readtext(file = here("data/EJ", "*.pdf"), 
                   docvarsfrom = "filenames", 
                   docvarnames = c("type", "subj", "year"),
                   sep = "_")
#creating an initial corpus containing our data
epa_corp <- corpus(x = ej_pdf, text_field = "text" )
summary(epa_corp) %>% 
  knitr::kable(caption = "Summary of EPA Reprot Corpus")
```

```{r}
# Add some additional, context-specific stop words to stop word lexicon
more_stops <-c("2015","2016", "2017", "2018", "2019", "2020", "www.epa.gov", "https")
add_stops<- tibble(word = c(stop_words$word, more_stops))
stop_vec <- as_vector(add_stops)
```

Create different data objects that will be used for the subsequent analyses
```{r tidy}
#convert to tidy format and apply my stop words
raw_text <- tidy(epa_corp)

#Distribution of most frequent words across documents
raw_words <- raw_text %>%
  mutate(year = as.factor(year)) %>%
  unnest_tokens(word, text) %>%
  anti_join(add_stops, by = 'word') %>%
  count(year, word, sort = TRUE)
```

```{r}
#number of total words by document  
total_words <- raw_words %>%
  group_by(year) %>%
  summarize(total = sum(n))

report_words <- left_join(raw_words, total_words)

par_tokens <- unnest_tokens(raw_text, output = paragraphs, input = text, token = "paragraphs")

par_tokens <- par_tokens %>%
 mutate(par_id = 1:n())

par_words <- unnest_tokens(par_tokens, output = word, input = paragraphs, token = "words")
```

```{r quanteda_init}
tokens <- tokens(epa_corp, remove_punct = TRUE)
toks1<- tokens_select(tokens, min_nchar = 3)
toks1 <- tokens_tolower(toks1)
toks1 <- tokens_remove(toks1, pattern = (stop_vec))
dfm <- dfm(toks1)
```

```{r}
#first the basic frequency stat
tstat_freq <- textstat_frequency(dfm, n = 5, groups = year)
head(tstat_freq, 15) %>% 
  knitr::kable(caption = "Subset of Top 5 Words")
```
# 1.  What are the most frequent trigrams in the dataset? How does this compare to the most frequent bigrams? Which n-gram seems more informative here, and why?
```{r bigrams}
# bigrams
toks2 <- tokens_ngrams(toks1, n=2)
dfm2 <- dfm(toks2) # document feature matrix
dfm2 <- dfm_remove(dfm2, pattern = c(stop_vec))
freq_words2 <- textstat_frequency(dfm2, n=20)
freq_words2$token <- rep("bigram", 20)
#tokens1 <- tokens_select(tokens1,pattern = stopwords("en"), selection = "remove")

bigrams <- freq_words2 %>%
  knitr::kable(caption = "Bigrams")
bigrams
```

```{r trigrams}
# trigrams
toks3 <- tokens_ngrams(toks1, n=3)
dfm3 <- dfm(toks3) # document feature matrix
dfm3 <- dfm_remove(dfm3, pattern = c(stop_vec))
freq_words3 <- textstat_frequency(dfm3, n=20)
freq_words3$token <- rep("trigram", 20)

trigrams <- freq_words3 %>%
  knitr::kable(caption = "Trigrams")
trigrams
```

The three most frequent trigrams are `r freq_words3[1]$feature`, `r freq_words3[2]$featur`, and `r freq_words3[3]$feature`. The three most frequent bigrams are `r freq_words2[1]$feature`, `r freq_words2[2]$featur`, and `r freq_words2[3]$feature`. The words 'environmental' and 'justice' appear several times in both the top bigrams and top trigrams. The bigrams seem more informative than the trigrams because there is more variety in the terms. Most of the trigrams are variations of 'environmental justice'. Also, the top trigrams are likely all part of the same phrase 'environmental justice fy2017 progress report'.

# 2.  Choose a new focal term to replace "justice" and recreate the correlation table and network (see corr_paragraphs and corr_network chunks). Explore some of the plotting parameters in the cor_network chunk to see if you can improve the clarity or amount of information your plot conveys. Make sure to use a different color for the ties!

```{r corr_paragraphs}
word_cors <- par_words %>% 
  add_count(par_id) %>% 
  filter(n >= 50) %>% 
  select(-n) %>%
  pairwise_cor(word, par_id, sort = TRUE)
```

```{r corr_network}
water_cors <- word_cors %>% 
  filter(item1 == "water")

  word_cors %>%
  filter(item1 %in% c("water", "air", "health", "public"))%>%
  group_by(item1) %>%
  top_n(6) %>%
  ungroup() %>%
  mutate(item1 = as.factor(item1),
  name = reorder_within(item2, correlation, item1)) %>%
  ggplot(aes(y = name, x = correlation, fill = item1)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~item1, ncol = 2, scales = "free")+
  scale_y_reordered() +
  labs(y = NULL,
         x = NULL,
         title = "Correlations with key words",
         subtitle = "EPA EJ Reports")
```

```{r}
#let's zoom in on just one of our key terms
water_cors <- word_cors %>%
  filter(item1 == "water") %>%
  mutate(n = 1:n())

water_cors  %>%
  filter(n <= 30) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation, edge_width = correlation), edge_colour = "steelblue3") +
  geom_node_point(size = 4) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void()
```

# 3.  Write a function that allows you to conduct a keyness analysis to compare two individual EPA reports (hint: that means target and reference need to both be individual reports). Run the function on 3 pairs of reports, generating 3 keyness plots.

```{r keyness function}
keyness_function <- function(reference_report_year, target_report_year) {
  files <- list.files(path = here("data/EJ"),
                    pattern = "pdf$", full.names = TRUE)
  ej_reports <- lapply(files, pdf_text)
  ej_pdf <- readtext(file = here("data/EJ", "*.pdf"), 
                   docvarsfrom = "filenames", 
                   docvarnames = c("type", "subj", "year"),
                   sep = "_")
  epa_corp <- corpus(x = ej_pdf, text_field = "text" )
  tokens <- tokens(epa_corp, remove_punct = TRUE)
  toks1<- tokens_select(tokens, min_nchar = 3)
  toks1 <- tokens_tolower(toks1)
  toks1 <- tokens_remove(toks1, pattern = (stop_vec))
  dfm <- dfm(toks1)
  
  keyness_function_plot <- dfm %>% 
    dfm_subset(year %in% c(reference_report_year, target_report_year)) %>% 
    textstat_keyness(target = paste0("EPA_EJ_", target_report_year, ".pdf")) %>% 
    textplot_keyness()
  keyness_function_plot
}
```

```{r}
# 2015 vs. 2016
keyness_function(reference_report_year = 2015, target_report_year = 2016)
```

```{r}
# 2017 vs. 2018
keyness_function(reference_report_year = 2017, target_report_year = 2018)
```

```{r}
# 2019 vs. 2020
keyness_function(reference_report_year = 2019, target_report_year = 2020)
# hey covid-2019 pandemic
```
Note:
EPA_EJ_2015.pdf is report for FY 2015 (published Sept. 2016)

EPA_EJ_2016.pdf is report for FY 2015-2016

EPA_EJ_2017.pdf is report for FY 2018

EPA_EJ_2018.pdf is report for FY 2017

EPA_EJ_2019.pdf is report for FY 2019

EPA_EJ_2020.pdf is report for FY 2020

# 4.  Select a word or multi-word term of interest and identify words related to it using windowing and keyness comparison. To do this you will create two objects: one containing all words occurring within a 10-word window of your term of interest, and the second object containing all other words. Then run a keyness comparison on these objects. Which one is the target, and which the reference? [Hint](https://tutorials.quanteda.io/advanced-operations/target-word-collocations/)
```{r}

```

